1) Старт процесса и базовая инициализация

main():

создаёт Platform (SDL-окно) и NetPoller.

вешает net-хук loop_hook_add_end_of_frame(...) → в конце каждого кадра вызывается net_poller_tick() (тонко «крутит» сеть, не блокируя UI).

инициализирует шрифт/текст.

создаёт WM и добавляет окна paint/square.

2) Создание консоли и её «мозгов»

Консольная М-V-С тройка:

ConsoleStore* con_store = con_store_create(); — состояние.

ConsoleProcessor* con_proc = con_processor_create(con_store); — логика, генерит ConOp’ы.

Регистрируется тип console в TypeRegistry с type_id=1:

init_from_blob() — восстановление состояния из снапшота,

apply() — применить приходящую операцию,

snapshot() — сериализовать текущее состояние (для «тёплого» подключения/переключения).

Делается ConsoleSink — мост между процессором и репликатором (спекуляция локально + подтверждение снаружи).

Процессору передают NetPoller (для команд «старт/коннект лидера» и т.п.).

3) Конструирование бэкендов и ReplHub

Создаются бэкенды:

replicator_create_leader_tcp(poller, console_id, 33334) — лидер-сервер (если возможно слушать порт).

replicator_create_local_loop() — всегда доступный in-proc fallback.

(опционально) replicator_create_crdt_mesh(poller, port, seeds, nseeds) — если ты его подключишь в main.c.

Бэкенды складываются в ReplHub c приоритетами (например, LEADER:100 > CRDT:50 > LOCAL:10).

Политика в хабе выбирает «лучший» здоровый бэкенд по приоритету и capabilities.

4) Привязка консоли к репликации

ConsoleSink подписывается в репликатор на свой TopicId{type=1, inst=console_id} через replicator_set_listener(...).

Хаб находит/создаёт маршрут для этого топика, выбирает лучший бэкенд и «перевешивает» listener на него.

Если это CRDT-бэкенд — он запоминает топик и будет отправлять снапшот вновь подключившимся пирами.

5) Главный цикл и доставка событий

Основной цикл кадра:

plat_poll_events_and_dispatch() собирает ввод, прокидывает в WM → WinConsole → ConsoleProcessor.

Процессор из ввода рождает ConOp (вставка текста, бэкспейс, команды и т.д.) и отдаёт в ConsoleSink.

ConsoleSink делает локальную «спекулятивную» применённость и зовёт replicator_publish(repl, &op).

ReplicatorHub.publish():

по TopicId выбирает маршрут; если надо — переоценивает «лучший» бэкенд (здоровье/приоритеты),

вызывает publish() конкретного бэкенда.

Контракт publish: бэкенд копирует payload’ы (можно слать асинхронно).

6) Что делают конкретные бэкенды при publish()

local_loop:

просто «фан-аутит» ConOp всем локальным подписчикам (listener’ам) → ConsoleSink получает подтверждение и прекращает «спекуляцию».

leader_tcp:

немедленно «фан-аутит» локальным listener’ам,

кодирует COW1-кадр и рассылает всем подключенным TCP-клиентам.

На стороне сервера net_poller_tick() крутит:

accept’ы на listen-fd,

handshake HELO/WLCM,

чтение/запись COW1 через Cow1Tcp.

crdt_mesh:

локальный «фан-аут»,

рассылка всем текущим пирами,

дедуп по (console_id, op_id) (включая зацикливание через пересылку от пира к пиру).

При новом соединении crdt_mesh отправляет снапшоты всех известных топиков (через TypeRegistry::snapshot()), чтобы пир мгновенно «приехал» в актуальное состояние.

7) Приём операций (confirm path)

Когда COW1-декодер (в leader_tcp или crdt_mesh) получает кадр от клиента/пира:

копирует payload’ы, делает дедуп (для CRDT),

«фан-аутит» локальным listener’ам → ConsoleSink видит подтверждение,

ретрансмитит дальше (leader всем клиентам, CRDT всем пирами кроме источника).

8) Роль сетевого хука

В конце каждого кадра вызывается net_poller_tick():

это двигает handshake TCP, приём/передачу COW1-кадров, коннекты к seed-узлам CRDT и т.д.

В wasm-сборке сетевая заглушка делает вызов «пустым», так что UI не блокируется.

9) Снапшоты и «тёплые» подключения

Источники снапшота:

CRDT-бэкенд: при появлении нового пира по всем подписанным топикам отправляет ConOp{ tag="snapshot", init_blob=... }.

Хаб при мягком переключении бэкенда (replhub_switch_backend()): берёт снапшот из TypeRegistry и «заливает» в новый бэкенд до разблокировки.

10) Мягкое переключение бэкенда (ручное или по команде)

Если пользователь/команда решает сменить схему (например, «перейти с LOCAL на LEADER» или на CRDT):

вызывается replhub_switch_backend(hub, topic, to_index_or_<0>).

Хаб:

блокирует publish() для этого топика и буферизует операции,

берёт снапшот и публикует в целевой бэкенд,

перевешивает listener,

разблокирует и «досылает» буфер по порядку.

В результате компонент продолжает работать через новый канал без потери операций.

11) Что будет в offline/online/Web

Offline/«нет сети»:

leader_tcp нездоров (listen не поднялся) → политика хаба выберет local_loop.

Online (есть порт/пиры):

как только leader_tcp или crdt_mesh становятся «здоровы», они попадают в пул кандидатов; при ручном переключении — мягко перейдёшь, при автопереоценке новый будет выбран для новых маршрутов (для уже активных — лучше вызвать replhub_switch_backend()).

WebAssembly:

leader_tcp и сетевой стек — заглушки, фактически останется только local_loop (или CRDT, если ты соберёшь web-специфичный транспорт в будущем).

12) Несколько компонентов и экземпляров

Любой новый компонент:

получает свой type_id, регистрирует TypeVt (init/apply/snapshot),

публикует/слушает по TopicId{ type_id, inst_id },

пользуется тем же Replicator интерфейсом (Hub + backend’ы), независимо от формы своего состояния.

Хаб ведёт отдельный маршрут на каждый TopicId, поэтому несколько экземпляров (например, две консоли) могут независимо выбирать/переключать бэкенды.

13) Завершение работы

На выход:

снимается net-хук,

разрушается WM, ConsoleProcessor/Sink/Store, Replicator (Hub тоже прибирает бэкенды, если был создан с adopt_backends=1), NetPoller, Platform, text_shutdown().

Если захочешь задействовать CRDT-схему прямо сейчас:

включи создание Replicator* b_crdt = replicator_create_crdt_mesh(poller, 33335, seeds, nseeds);

добавь его в массив backends с приоритетом между LEADER и LOCAL (например, 50),

при необходимости сделай команды в консоли для «поднять listen»/«подключиться к seed» и «переключиться на CRDT» → пусть вызывают replhub_switch_backend(...) для нужного TopicId.
